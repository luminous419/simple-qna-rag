#!/usr/bin/env python3
"""
ë¬¸ì„œ ë“±ë¡ í”„ë¡œê·¸ë¨

data ë””ë ‰í† ë¦¬ì˜ PDF ë° í…ìŠ¤íŠ¸ ë¬¸ì„œë¥¼ ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥í•˜ëŠ” ë°°ì¹˜ í”„ë¡œê·¸ë¨
"""

import os
import sys
import time
import threading
from pathlib import Path
from typing import List

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader, TextLoader, DirectoryLoader
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings

from config import (
    EMBEDDING_MODEL_NAME,
    VECTORSTORE_PATH,
    DATA_DIR,
    CHUNK_SIZE,
    CHUNK_OVERLAP,
    NORMALIZE_EMBEDDINGS,
)


def load_documents() -> List:
    """
    data ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  PDFì™€ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.

    Returns:
        List: ë¡œë“œëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    """
    print(f"ğŸ“‚ ë¬¸ì„œ ë¡œë”© ì¤‘: {DATA_DIR}")

    if not os.path.exists(DATA_DIR):
        print(f"âš ï¸  ê²½ê³ : {DATA_DIR} ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        os.makedirs(DATA_DIR, exist_ok=True)
        print(f"âœ… {DATA_DIR} ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
        print(f"â„¹ï¸  ë¬¸ì„œë¥¼ {DATA_DIR}ì— ì¶”ê°€í•˜ê³  ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return []

    documents = []

    # PDF íŒŒì¼ ë¡œë“œ
    try:
        pdf_loader = DirectoryLoader(
            DATA_DIR,
            glob="**/*.pdf",
            loader_cls=PyPDFLoader,
            show_progress=True,
        )
        pdf_docs = pdf_loader.load()
        documents.extend(pdf_docs)
        print(f"âœ… PDF íŒŒì¼ {len(pdf_docs)}ê°œ ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸  PDF ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}")

    # í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ
    try:
        text_loader = DirectoryLoader(
            DATA_DIR,
            glob="**/*.txt",
            loader_cls=TextLoader,
            show_progress=True,
        )
        text_docs = text_loader.load()
        documents.extend(text_docs)
        print(f"âœ… í…ìŠ¤íŠ¸ íŒŒì¼ {len(text_docs)}ê°œ ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸  í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}")

    print(f"ğŸ“Š ì´ {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
    return documents


def split_documents(documents: List) -> List:
    """
    ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.

    Args:
        documents: ë¶„í• í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸

    Returns:
        List: ë¶„í• ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
    """
    print(f"âœ‚ï¸  ë¬¸ì„œ ë¶„í•  ì¤‘ (ì²­í¬ í¬ê¸°: {CHUNK_SIZE}, ì˜¤ë²„ë©: {CHUNK_OVERLAP})")

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        length_function=len,
        separators=["\n\n", "\n", ". ", " ", ""],
    )

    chunks = text_splitter.split_documents(documents)
    print(f"âœ… {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ")

    return chunks


class ProgressTracker:
    """ì„ë² ë”© ì§„í–‰ ìƒí™©ì„ ì¶”ì í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, total_items: int):
        self.total_items = total_items
        self.processed_items = 0
        self.start_time = None
        self.stop_flag = False
        self.lock = threading.Lock()

    def start(self):
        """ì§„í–‰ í‘œì‹œ ì‹œì‘"""
        self.start_time = time.time()
        self.stop_flag = False

        def print_progress():
            """1ì´ˆë§ˆë‹¤ dot(.)ê³¼ ì§„í–‰ë¥  ì¶œë ¥"""
            last_percentage = -1
            while not self.stop_flag:
                with self.lock:
                    if self.processed_items > 0:
                        percentage = int((self.processed_items / self.total_items) * 100)

                        # í¼ì„¼íŠ¸ê°€ ë³€ê²½ë˜ì—ˆì„ ë•Œë§Œ ì¶œë ¥
                        if percentage != last_percentage:
                            elapsed = time.time() - self.start_time
                            print(f"\rì§„í–‰: {'.' * (percentage // 5)} {percentage}% ({self.processed_items}/{self.total_items} ì²­í¬) - {elapsed:.1f}ì´ˆ ê²½ê³¼", end='', flush=True)
                            last_percentage = percentage

                time.sleep(1)

        # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ì§„í–‰ë¥  ì¶œë ¥
        self.thread = threading.Thread(target=print_progress, daemon=True)
        self.thread.start()

    def update(self, processed: int):
        """ì²˜ë¦¬ëœ ì•„ì´í…œ ìˆ˜ ì—…ë°ì´íŠ¸"""
        with self.lock:
            self.processed_items = processed

    def stop(self):
        """ì§„í–‰ í‘œì‹œ ì¤‘ì§€"""
        self.stop_flag = True
        if hasattr(self, 'thread'):
            self.thread.join(timeout=2)
        print()  # ìƒˆ ì¤„ë¡œ ì´ë™


def create_vectorstore(chunks: List) -> FAISS:
    """
    ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•˜ê³  ì²­í¬ë¥¼ ì„ë² ë”©í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.
    L2 ì •ê·œí™” + FAISS IndexFlatIP (Inner Product) ì‚¬ìš©

    Args:
        chunks: ì €ì¥í•  ì²­í¬ ë¦¬ìŠ¤íŠ¸

    Returns:
        FAISS: ìƒì„±ëœ ë²¡í„°ìŠ¤í† ì–´
    """
    print(f"ğŸ”§ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘: {EMBEDDING_MODEL_NAME}")
    print(f"   - L2 ì •ê·œí™”: {NORMALIZE_EMBEDDINGS}")

    # HuggingFace ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL_NAME,
        model_kwargs={'device': 'cpu'},  # GPU ì‚¬ìš© ì‹œ 'cuda'ë¡œ ë³€ê²½
        encode_kwargs={'normalize_embeddings': NORMALIZE_EMBEDDINGS}  # L2 ì •ê·œí™”
    )

    print(f"ğŸ’¾ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘ (FAISS IndexFlatIP): {VECTORSTORE_PATH}")
    print(f"ğŸ“Š ì´ {len(chunks)}ê°œ ì²­í¬ ì„ë² ë”© ì‹œì‘...")

    # ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ê°€ ìˆë‹¤ë©´ ì‚­ì œ
    if os.path.exists(VECTORSTORE_PATH):
        print(f"âš ï¸  ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤: {VECTORSTORE_PATH}")
        import shutil
        shutil.rmtree(VECTORSTORE_PATH)

    # Progress Tracker ì‹œì‘
    progress = ProgressTracker(len(chunks))
    progress.start()

    try:
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ë² ë”©í•˜ì—¬ ì§„í–‰ë¥  ì¶”ì 
        batch_size = 10  # ë°°ì¹˜ í¬ê¸°
        all_embeddings = []

        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i + batch_size]
            batch_texts = [chunk.page_content for chunk in batch]

            # ë°°ì¹˜ ì„ë² ë”©
            batch_embeds = embeddings.embed_documents(batch_texts)
            all_embeddings.extend(batch_embeds)

            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            progress.update(min(i + batch_size, len(chunks)))

        progress.stop()

        # FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        print("ğŸ”§ FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        import numpy as np
        from langchain_core.documents import Document

        # ì„ë² ë”©ì„ numpy ë°°ì—´ë¡œ ë³€í™˜
        embeddings_array = np.array(all_embeddings).astype('float32')

        # FAISS ì¸ë±ìŠ¤ ìƒì„±
        import faiss
        dimension = embeddings_array.shape[1]
        index = faiss.IndexFlatIP(dimension)  # Inner Product ì¸ë±ìŠ¤
        index.add(embeddings_array)

        # FAISS vectorstore ìƒì„±
        from langchain_community.docstore.in_memory import InMemoryDocstore

        # ë¬¸ì„œ ID ë§¤í•‘
        index_to_id = {i: str(i) for i in range(len(chunks))}
        docstore = InMemoryDocstore({str(i): chunks[i] for i in range(len(chunks))})

        vectorstore = FAISS(
            embedding_function=embeddings,
            index=index,
            docstore=docstore,
            index_to_docstore_id=index_to_id
        )

    except Exception as e:
        progress.stop()
        raise e

    # ë¡œì»¬ ë””ë ‰í† ë¦¬ì— ì €ì¥
    print("ğŸ’¾ ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ ì¤‘...")
    os.makedirs(VECTORSTORE_PATH, exist_ok=True)
    vectorstore.save_local(VECTORSTORE_PATH)

    print(f"âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ì €ì¥ë¨")
    print(f"   - ì €ì¥ ìœ„ì¹˜: {VECTORSTORE_PATH}")
    print(f"   - ì¸ë±ìŠ¤ íƒ€ì…: FAISS IndexFlatIP (ì •ê·œí™”ëœ ë²¡í„° + Inner Product)")

    return vectorstore


def main():
    """
    ë©”ì¸ í•¨ìˆ˜: ë¬¸ì„œ ë¡œë“œ -> ë¶„í•  -> ë²¡í„°ìŠ¤í† ì–´ ìƒì„± -> ì €ì¥
    """
    print("=" * 60)
    print("ğŸ“š ë¬¸ì„œ ë“±ë¡ í”„ë¡œê·¸ë¨ ì‹œì‘")
    print("=" * 60)

    # 1. ë¬¸ì„œ ë¡œë“œ
    documents = load_documents()

    if not documents:
        print("\nâš ï¸  ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        print(f"â„¹ï¸  {DATA_DIR} ë””ë ‰í† ë¦¬ì— PDF ë˜ëŠ” TXT íŒŒì¼ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
        sys.exit(0)

    # 2. ë¬¸ì„œ ë¶„í• 
    chunks = split_documents(documents)

    if not chunks:
        print("\nâš ï¸  ë¬¸ì„œ ë¶„í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    # 3. ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ë° ì €ì¥
    try:
        vectorstore = create_vectorstore(chunks)

        print("\n" + "=" * 60)
        print("âœ… ë¬¸ì„œ ë“±ë¡ ì™„ë£Œ!")
        print("=" * 60)
        print(f"ğŸ“Š í†µê³„:")
        print(f"  - ì´ ë¬¸ì„œ ìˆ˜: {len(documents)}")
        print(f"  - ì´ ì²­í¬ ìˆ˜: {len(chunks)}")
        print(f"  - ì €ì¥ ìœ„ì¹˜: {VECTORSTORE_PATH}")
        print(f"  - ì„ë² ë”© ëª¨ë¸: {EMBEDDING_MODEL_NAME}")
        print("\nğŸ’¡ ì´ì œ document_query.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ì§ˆì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
