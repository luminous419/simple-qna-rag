#!/usr/bin/env python3
"""
RAG ì½”ì–´ ì—”ì§„

ë²¡í„°ìŠ¤í† ì–´, LLM, ê²€ìƒ‰ ì²´ì¸ì„ ê´€ë¦¬í•˜ëŠ” í•µì‹¬ ëª¨ë“ˆ
í•œ ë²ˆ ì´ˆê¸°í™”í•˜ë©´ ì „ì—­ìœ¼ë¡œ ì¬ì‚¬ìš© ê°€ëŠ¥
"""

import os
import sys
from typing import Optional, Dict, List

from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import OllamaLLM

from config import (
    EMBEDDING_MODEL_NAME,
    VECTORSTORE_PATH,
    OLLAMA_BASE_URL,
    OLLAMA_MODEL,
    RETRIEVAL_K,
    USE_MMR,
    MMR_FETCH_K,
    MMR_K,
    MMR_LAMBDA,
    NORMALIZE_EMBEDDINGS,
    USE_HYBRID_SEARCH,
    BM25_TOP_K,
    DENSE_TOP_K,
    RRF_TOP_K,
    RRF_CONSTANT,
    USE_RERANKER,
    RERANKER_MODEL,
    RERANKER_TOP_K,
    PROMPT_TEMPLATE,
)


class RAGEngine:
    """RAG ì—”ì§„ ì‹±ê¸€í†¤ í´ë˜ìŠ¤"""

    _instance = None
    _initialized = False

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(RAGEngine, cls).__new__(cls)
        return cls._instance

    def __init__(self):
        """ì´ˆê¸°í™”ëŠ” í•œ ë²ˆë§Œ ìˆ˜í–‰"""
        if not RAGEngine._initialized:
            self.vectorstore = None
            self.bm25_retriever = None
            self.dense_retriever = None
            self.llm = None
            self.qa_chain = None
            RAGEngine._initialized = True

    def initialize(self) -> bool:
        """
        RAG ì—”ì§„ ì´ˆê¸°í™”

        Returns:
            bool: ì´ˆê¸°í™” ì„±ê³µ ì—¬ë¶€
        """
        try:
            print("=" * 60)
            print("ğŸš€ RAG ì—”ì§„ ì´ˆê¸°í™” ì‹œì‘")
            print("=" * 60)

            # 1. ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ
            self.vectorstore = self._load_vectorstore()

            # 2. BM25 ê²€ìƒ‰ê¸° ìƒì„± (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‚¬ìš© ì‹œ)
            if USE_HYBRID_SEARCH:
                print(f"\nğŸ”§ BM25 ê²€ìƒ‰ê¸° ìƒì„± ì¤‘...")
                self.bm25_retriever = self._create_bm25_retriever(self.vectorstore)

            # 3. LLM ì´ˆê¸°í™”
            self.llm = self._initialize_llm()

            # 4. QA ì²´ì¸ ì„¤ì •
            self.dense_retriever, self.qa_chain = self._setup_qa_chain()

            print("\n" + "=" * 60)
            print("âœ… RAG ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
            print("=" * 60)
            return True

        except Exception as e:
            print(f"âŒ RAG ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False

    def _load_vectorstore(self) -> FAISS:
        """ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ"""
        print(f"ğŸ“‚ ë²¡í„°ìŠ¤í† ì–´ ë¡œë”© ì¤‘: {VECTORSTORE_PATH}")

        if not os.path.exists(VECTORSTORE_PATH):
            raise FileNotFoundError(
                f"ë²¡í„°ìŠ¤í† ì–´ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {VECTORSTORE_PATH}\n"
                f"ë¨¼ì € document_register.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ë¬¸ì„œë¥¼ ë“±ë¡í•´ì£¼ì„¸ìš”."
            )

        embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL_NAME,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': NORMALIZE_EMBEDDINGS}
        )

        vectorstore = FAISS.load_local(
            VECTORSTORE_PATH,
            embeddings,
            allow_dangerous_deserialization=True
        )

        print(f"âœ… ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ (FAISS IndexFlatIP)")
        return vectorstore

    def _create_bm25_retriever(self, vectorstore: FAISS):
        """BM25 ê²€ìƒ‰ê¸° ìƒì„±"""
        from rank_bm25 import BM25Okapi

        all_docs = list(vectorstore.docstore._dict.values())
        tokenized_docs = [doc.page_content.split() for doc in all_docs]
        bm25 = BM25Okapi(tokenized_docs)

        print(f"âœ… BM25 ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ (ë¬¸ì„œ {len(all_docs)}ê°œ)")

        class BM25Retriever:
            def __init__(self, bm25_index, documents):
                self.bm25 = bm25_index
                self.documents = documents

            def invoke(self, query: str, top_k: int = 50):
                tokenized_query = query.split()
                scores = self.bm25.get_scores(tokenized_query)
                scored_docs = list(zip(self.documents, scores))
                scored_docs.sort(key=lambda x: x[1], reverse=True)
                return [doc for doc, score in scored_docs[:top_k]]

        return BM25Retriever(bm25, all_docs)

    def _initialize_llm(self):
        """LLM ì´ˆê¸°í™”"""
        print(f"ğŸ”§ LLM ì´ˆê¸°í™” ì¤‘: {OLLAMA_MODEL}")
        print(f"â„¹ï¸  Ollamaê°€ ì‹¤í–‰ ì¤‘ì´ê³  {OLLAMA_MODEL} ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.")
        print(f"â„¹ï¸  OLLAMA_BASE_URL: {OLLAMA_BASE_URL}")

        llm = OllamaLLM(
            model=OLLAMA_MODEL,
            base_url=OLLAMA_BASE_URL,
            temperature=0.1,
        )

        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
        _ = llm.invoke("test")
        print(f"âœ… LLM ì´ˆê¸°í™” ì™„ë£Œ")

        return llm

    def _setup_qa_chain(self):
        """QA ì²´ì¸ ì„¤ì •"""
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        prompt = PromptTemplate(
            template=PROMPT_TEMPLATE,
            input_variables=["context", "question"]
        )

        # Dense Retriever ì„¤ì •
        if USE_HYBRID_SEARCH:
            print(f"ğŸ” Stage 1 - Hybrid Search ì„¤ì •")
            print(f"   - Dense (FAISS): {DENSE_TOP_K}ê°œ")
            print(f"   - Sparse (BM25): {BM25_TOP_K}ê°œ")
            print(f"   - RRF ìœµí•© í›„: {RRF_TOP_K}ê°œ")

            dense_retriever = self.vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": DENSE_TOP_K}
            )
        elif USE_MMR:
            print(f"ğŸ” Stage 1 - Retriever ì„¤ì •: MMR (ë‹¤ì–‘ì„± í™•ë³´)")
            print(f"   - k={MMR_K}, fetch_k={MMR_FETCH_K}, lambda={MMR_LAMBDA}")
            dense_retriever = self.vectorstore.as_retriever(
                search_type="mmr",
                search_kwargs={
                    "k": MMR_K,
                    "fetch_k": MMR_FETCH_K,
                    "lambda_mult": MMR_LAMBDA
                }
            )
        else:
            print(f"ğŸ” Stage 1 - Retriever ì„¤ì •: Similarity (ìœ ì‚¬ë„)")
            print(f"   - k={RETRIEVAL_K}")
            dense_retriever = self.vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": RETRIEVAL_K}
            )

        # Re-ranker ì„¤ì •
        if USE_RERANKER:
            print(f"ğŸ” Stage 2 - Re-ranker í™œì„±í™”")
            print(f"   - ëª¨ë¸: {RERANKER_MODEL}")
            print(f"   - ìµœì¢… ë¬¸ì„œ ìˆ˜: {RERANKER_TOP_K}")
            if USE_HYBRID_SEARCH:
                print(f"   - íŒŒì´í”„ë¼ì¸: BM25+FAISS({BM25_TOP_K}+{DENSE_TOP_K}ê°œ) â†’ RRF({RRF_TOP_K}ê°œ) â†’ Re-rank({RERANKER_TOP_K}ê°œ)")
            else:
                print(f"   - íŒŒì´í”„ë¼ì¸: FAISS({MMR_K if USE_MMR else RETRIEVAL_K}ê°œ) â†’ Re-rank({RERANKER_TOP_K}ê°œ)")

        # QA ì²´ì¸ ìƒì„±
        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)

        if USE_HYBRID_SEARCH:
            def hybrid_retrieve_and_rerank(query: str):
                bm25_docs = self.bm25_retriever.invoke(query, top_k=BM25_TOP_K)
                dense_docs = dense_retriever.invoke(query)
                fused_docs = self._reciprocal_rank_fusion(
                    bm25_docs, dense_docs,
                    top_k=RRF_TOP_K,
                    k=RRF_CONSTANT
                )
                if USE_RERANKER:
                    return self._rerank_documents(query, fused_docs, top_k=RERANKER_TOP_K)
                return fused_docs

            qa_chain = (
                {"context": RunnableLambda(hybrid_retrieve_and_rerank) | RunnableLambda(format_docs), "question": RunnablePassthrough()}
                | prompt
                | self.llm
                | StrOutputParser()
            )

        elif USE_RERANKER:
            def retrieve_and_rerank(query: str):
                docs = dense_retriever.invoke(query)
                return self._rerank_documents(query, docs, top_k=RERANKER_TOP_K)

            qa_chain = (
                {"context": RunnableLambda(retrieve_and_rerank) | RunnableLambda(format_docs), "question": RunnablePassthrough()}
                | prompt
                | self.llm
                | StrOutputParser()
            )

        else:
            qa_chain = (
                {"context": dense_retriever | RunnableLambda(format_docs), "question": RunnablePassthrough()}
                | prompt
                | self.llm
                | StrOutputParser()
            )

        print(f"âœ… QA ì²´ì¸ ì„¤ì • ì™„ë£Œ")
        return dense_retriever, qa_chain

    def _reciprocal_rank_fusion(self, bm25_docs, dense_docs, top_k: int = 20, k: int = 60):
        """RRF ìœµí•©"""
        from collections import defaultdict

        rrf_scores = defaultdict(float)

        for rank, doc in enumerate(bm25_docs, start=1):
            rrf_scores[id(doc)] += 1.0 / (k + rank)

        for rank, doc in enumerate(dense_docs, start=1):
            rrf_scores[id(doc)] += 1.0 / (k + rank)

        all_docs = {}
        for doc in bm25_docs + dense_docs:
            all_docs[id(doc)] = doc

        sorted_docs = sorted(
            all_docs.items(),
            key=lambda x: rrf_scores[x[0]],
            reverse=True
        )

        return [doc for doc_id, doc in sorted_docs[:top_k]]

    def _rerank_documents(self, query: str, documents, top_k: int = 5):
        """Cross-Encoder Re-ranking"""
        from sentence_transformers import CrossEncoder

        if not hasattr(self, 'reranker_model'):
            print(f"ğŸ”§ Re-ranker ëª¨ë¸ ë¡œë”© ì¤‘: {RERANKER_MODEL}")
            self.reranker_model = CrossEncoder(RERANKER_MODEL, max_length=512)
            print(f"âœ… Re-ranker ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

        pairs = [[query, doc.page_content] for doc in documents]
        scores = self.reranker_model.predict(pairs)
        scored_docs = list(zip(documents, scores))
        scored_docs.sort(key=lambda x: x[1], reverse=True)

        return [doc for doc, score in scored_docs[:top_k]]

    def query(self, question: str) -> Dict[str, any]:
        """
        ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸

        Returns:
            dict: {
                "answer": str,  # ë‹µë³€
                "sources": List[Dict],  # ì¶œì²˜ ë¬¸ì„œ
                "success": bool
            }
        """
        try:
            if not self.qa_chain:
                raise RuntimeError("RAG ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            # ë‹µë³€ ìƒì„±
            answer = self.qa_chain.invoke(question)

            # ì¶œì²˜ ë¬¸ì„œ ê²€ìƒ‰
            if USE_HYBRID_SEARCH:
                bm25_docs = self.bm25_retriever.invoke(question, top_k=BM25_TOP_K)
                dense_docs = self.dense_retriever.invoke(question) if self.dense_retriever else []
                source_docs = self._reciprocal_rank_fusion(
                    bm25_docs, dense_docs,
                    top_k=RRF_TOP_K,
                    k=RRF_CONSTANT
                )
                if USE_RERANKER:
                    source_docs = self._rerank_documents(question, source_docs, top_k=RERANKER_TOP_K)
            elif USE_RERANKER:
                source_docs = self.dense_retriever.invoke(question)
                source_docs = self._rerank_documents(question, source_docs, top_k=RERANKER_TOP_K)
            else:
                source_docs = self.dense_retriever.invoke(question)

            # ì¶œì²˜ ì •ë³´ í¬ë§·
            sources = []
            for i, doc in enumerate(source_docs, 1):
                source = doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
                page = doc.metadata.get('page', None)
                sources.append({
                    "index": i,
                    "source": source,
                    "page": page + 1 if page is not None else None,
                    "content": doc.page_content[:200]  # ì²˜ìŒ 200ìë§Œ
                })

            return {
                "answer": answer,
                "sources": sources,
                "success": True
            }

        except Exception as e:
            print(f"âŒ ì§ˆì˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return {
                "answer": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "sources": [],
                "success": False
            }


# ì „ì—­ RAG ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤
_rag_engine = None


def get_rag_engine() -> RAGEngine:
    """RAG ì—”ì§„ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _rag_engine
    if _rag_engine is None:
        _rag_engine = RAGEngine()
        if not _rag_engine.initialize():
            raise RuntimeError("RAG ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨")
    return _rag_engine
